# Docker Compose configuration with AI Service
services:
  db:
    image: pgvector/pgvector:0.8.0-pg17
    container_name: echo_note_pgvector
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: echo_note_rag
    volumes:
      - pgdata:/var/lib/postgresql/data
    ports:
      - "5433:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres -d echo_note_rag"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - echo_notes_network

  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
      target: production
    container_name: echo_note_fastapi_main
    depends_on:
      db:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@db:5432/echo_note_rag
      JWT_SECRET_KEY: ${JWT_SECRET_KEY:-production-secret-key-change-me}
      # AI Service Configuration
      AI_SERVICE_URL: http://ai_service:8002
      # AWS S3 Configuration
      AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID:-}
      AWS_SECRET_ACCESS_KEY: ${AWS_SECRET_ACCESS_KEY:-}
      AWS_REGION: ${AWS_REGION:-ap-south-1}
      S3_BUCKET_NAME: ${S3_BUCKET_NAME:-bucket-of-echo}
      S3_ENDPOINT_URL: ${S3_ENDPOINT_URL:-}
    ports:
      - "8001:8000"
    restart: always
    networks:
      - echo_notes_network

  ai_service:
    build:
      context: .
      dockerfile: ai_service/Dockerfile
    container_name: echo_note_ai_service
    depends_on:
      db:
        condition: service_healthy
    environment:
      # Database Configuration (shared with main backend)
      DATABASE_URL: postgresql+asyncpg://postgres:postgres@db:5432/echo_note_rag

      # AI Service Configuration
      AI_SERVICE_HOST: 0.0.0.0
      AI_SERVICE_PORT: 8002
      AI_SERVICE_DEBUG: false
      AI_SERVICE_BACKEND_SERVICE_URL: http://backend:8000

      # OpenAI Configuration
      AI_SERVICE_OPENAI_API_KEY: ${OPENAI_API_KEY:-}
      AI_SERVICE_OPENAI_MODEL: ${OPENAI_MODEL:-gpt-4o-mini}
      AI_SERVICE_OPENAI_EMBEDDING_MODEL: ${OPENAI_EMBEDDING_MODEL:-text-embedding-3-small}
      AI_SERVICE_TEMPERATURE: 0.1

      # HuggingFace Configuration
      AI_SERVICE_HUGGINGFACE_API_TOKEN: ${HUGGINGFACEHUB_API_TOKEN:-}
      AI_SERVICE_HUGGINGFACE_MODEL_ID: ${HUGGINGFACE_MODEL_ID:-aaditya/Llama3-OpenBioLLM-70B}

      # NER Configuration
      AI_SERVICE_NER_MODEL_NAME: ${NER_MODEL_NAME:-d4data/biomedical-ner-all}

      # Model Caching Configuration
      AI_SERVICE_TRANSFORMERS_CACHE: /app/.cache/huggingface
      AI_SERVICE_HF_HOME: /app/.cache/huggingface
      AI_SERVICE_TORCH_HOME: /app/.cache/torch

      # Performance Configuration
      AI_SERVICE_MAX_CONCURRENT_REQUESTS: 10
      AI_SERVICE_REQUEST_TIMEOUT: 300

      # Logging Configuration
      AI_SERVICE_LOG_LEVEL: INFO
    ports:
      - "8002:8002"
    volumes:
      # Mount cache directories for model persistence
      - ai_cache:/app/.cache
    restart: always
    networks:
      - echo_notes_network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8002/health"]
      interval: 30s
      timeout: 30s
      start_period: 120s # Give more time for model loading
      retries: 3

volumes:
  pgdata:
  ai_cache:

networks:
  echo_notes_network:
    driver: bridge
